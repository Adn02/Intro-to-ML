{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b79e1ed7-efe5-413e-ab1f-fff793fdc529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import time\n",
    "\n",
    "datapath = '../data-unversions/p1ch7/'\n",
    "cifar10 = datasets.CIFAR10(root= datapath, train=True, download = True, transform=transforms.ToTensor())\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0791f4-7c5c-4225-87b0-dbaef5a376fe",
   "metadata": {},
   "source": [
    "Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f40ecb5-c449-4323-be58-83414022f88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "imgs = torch.stack([img_t for img_t, _ in cifar10], dim=3)\n",
    "mean = imgs.view(3, -1).mean(dim=1)\n",
    "std = imgs.view(3, -1).std(dim=1)\n",
    "\n",
    "normalize = transforms.Compose([transforms.ToTensor(),transforms.Normalize(mean,std)])\n",
    "\n",
    "train_data = datasets.CIFAR10(root= datapath, train=True, download = True, transform=normalize)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "\n",
    "valid_data = datasets.CIFAR10(root= datapath, train=False, download = True, transform=normalize)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23250901-33aa-429b-a4d6-1b076724a6c6",
   "metadata": {},
   "source": [
    "ResNet Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43e6aa66-d31e-4f1f-96f6-7ca038c23084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300, Loss: 1.6481623974297663, Training Time: 11.69 seconds\n",
      "Epoch 11/300, Loss: 0.8687803607310176, Training Time: 12.35 seconds\n",
      "Epoch 21/300, Loss: 0.7566721458797869, Training Time: 11.75 seconds\n",
      "Epoch 31/300, Loss: 0.695978500692131, Training Time: 11.86 seconds\n",
      "Epoch 41/300, Loss: 0.6510343938837271, Training Time: 11.80 seconds\n",
      "Epoch 51/300, Loss: 0.6135639289151067, Training Time: 11.79 seconds\n",
      "Epoch 61/300, Loss: 0.5938663480379393, Training Time: 11.83 seconds\n",
      "Epoch 71/300, Loss: 0.5699063487674879, Training Time: 11.85 seconds\n",
      "Epoch 81/300, Loss: 0.55032846737472, Training Time: 11.75 seconds\n",
      "Epoch 91/300, Loss: 0.5343578697546668, Training Time: 11.73 seconds\n",
      "Epoch 101/300, Loss: 0.5197022117845848, Training Time: 11.71 seconds\n",
      "Epoch 111/300, Loss: 0.5062748726905154, Training Time: 11.75 seconds\n",
      "Epoch 121/300, Loss: 0.49242198169993623, Training Time: 11.77 seconds\n",
      "Epoch 131/300, Loss: 0.48161202880656323, Training Time: 11.73 seconds\n",
      "Epoch 141/300, Loss: 0.4700854373405047, Training Time: 11.73 seconds\n",
      "Epoch 151/300, Loss: 0.46403708831047463, Training Time: 11.75 seconds\n",
      "Epoch 161/300, Loss: 0.4610067488211195, Training Time: 11.67 seconds\n",
      "Epoch 171/300, Loss: 0.44866724209407405, Training Time: 11.81 seconds\n",
      "Epoch 181/300, Loss: 0.44351809079308646, Training Time: 11.79 seconds\n",
      "Epoch 191/300, Loss: 0.4321170165906172, Training Time: 11.74 seconds\n",
      "Epoch 201/300, Loss: 0.422384386572539, Training Time: 11.81 seconds\n",
      "Epoch 211/300, Loss: 0.418055740349433, Training Time: 11.81 seconds\n",
      "Epoch 221/300, Loss: 0.4112682505070096, Training Time: 11.73 seconds\n",
      "Epoch 231/300, Loss: 0.4067563571397911, Training Time: 11.75 seconds\n",
      "Epoch 241/300, Loss: 0.40274401557872364, Training Time: 11.74 seconds\n",
      "Epoch 251/300, Loss: 0.3985027935155822, Training Time: 11.82 seconds\n",
      "Epoch 261/300, Loss: 0.39175899570707773, Training Time: 11.76 seconds\n",
      "Epoch 271/300, Loss: 0.38751400502212824, Training Time: 11.72 seconds\n",
      "Epoch 281/300, Loss: 0.3833131432685706, Training Time: 11.80 seconds\n",
      "Epoch 291/300, Loss: 0.3802445758410427, Training Time: 11.76 seconds\n",
      "Validation Accuracy: 68.86%\n"
     ]
    }
   ],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, inputs, outputs, stride=1):\n",
    "        super(Block, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inputs, outputs, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(outputs)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(outputs, outputs, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(outputs)\n",
    "        self.downsample = nn.Sequential()\n",
    "        if stride != 1 or inputs != outputs:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(inputs, outputs, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(outputs),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += self.downsample(identity)\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet10(nn.Module):\n",
    "    def __init__(self, block, layers):\n",
    "        super(ResNet10, self).__init__()\n",
    "        self.inputs = 16\n",
    "        self.conv1 = nn.Conv2d(3, self.inputs, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(self.inputs)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.blocks = self._make_layers(block, layers)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(self.inputs, 10)\n",
    "\n",
    "    def _make_layers(self, block, layers):\n",
    "        layers_list = []\n",
    "        for layer in layers:\n",
    "            layers_list.append(block(self.inputs, self.inputs, stride=1))\n",
    "        return nn.Sequential(*layers_list)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.blocks(x)\n",
    "        x = self.avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "model = ResNet10(Block, [2, 2, 2]).to(device)  \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Training loop\n",
    "epochs = 300\n",
    "total_start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    losses = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "        \n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {losses/len(train_loader)}, Training Time: {training_time:.2f} seconds')\n",
    "\n",
    "total_end_time = time.time()\n",
    "total_training_time = total_end_time - total_start_time\n",
    "\n",
    "# Validation\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in valid_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "valid_accuracy = correct / total\n",
    "print(f'Validation Accuracy: {valid_accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3ab141-ea6d-4b01-9cc8-d7edbae4063e",
   "metadata": {},
   "source": [
    "Weight Decay ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b75da62-7ddd-42d7-90d8-94f76288579e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300, Loss: 1.6594918667507903, Training Time: 11.84 seconds\n",
      "Epoch 11/300, Loss: 0.9126898744679472, Training Time: 12.45 seconds\n",
      "Epoch 21/300, Loss: 0.8407893780704654, Training Time: 11.46 seconds\n",
      "Epoch 31/300, Loss: 0.8043648933663088, Training Time: 11.50 seconds\n",
      "Epoch 41/300, Loss: 0.7768550750315951, Training Time: 11.53 seconds\n",
      "Epoch 51/300, Loss: 0.76492087866949, Training Time: 11.52 seconds\n",
      "Epoch 61/300, Loss: 0.7502386549397198, Training Time: 11.53 seconds\n",
      "Epoch 71/300, Loss: 0.7431146243344182, Training Time: 11.47 seconds\n",
      "Epoch 81/300, Loss: 0.7434568150955088, Training Time: 11.48 seconds\n",
      "Epoch 91/300, Loss: 0.7335436174555507, Training Time: 11.52 seconds\n",
      "Epoch 101/300, Loss: 0.728727473246167, Training Time: 11.55 seconds\n",
      "Epoch 111/300, Loss: 0.7261305977697567, Training Time: 11.51 seconds\n",
      "Epoch 121/300, Loss: 0.7175085701982079, Training Time: 11.48 seconds\n",
      "Epoch 131/300, Loss: 0.7181389949968099, Training Time: 11.47 seconds\n",
      "Epoch 141/300, Loss: 0.7066734560462825, Training Time: 11.52 seconds\n",
      "Epoch 151/300, Loss: 0.7058337569008093, Training Time: 11.53 seconds\n",
      "Epoch 161/300, Loss: 0.7115533031008737, Training Time: 11.54 seconds\n",
      "Epoch 171/300, Loss: 0.7050011279561635, Training Time: 11.51 seconds\n",
      "Epoch 181/300, Loss: 0.7042201120225365, Training Time: 11.47 seconds\n",
      "Epoch 191/300, Loss: 0.7033043397052209, Training Time: 11.56 seconds\n",
      "Epoch 201/300, Loss: 0.7005236631311724, Training Time: 11.53 seconds\n",
      "Epoch 211/300, Loss: 0.7030625599424553, Training Time: 11.51 seconds\n",
      "Epoch 221/300, Loss: 0.6998036932533659, Training Time: 11.58 seconds\n",
      "Epoch 231/300, Loss: 0.6974061679504716, Training Time: 11.47 seconds\n",
      "Epoch 241/300, Loss: 0.6986736361785313, Training Time: 11.54 seconds\n",
      "Epoch 251/300, Loss: 0.6929759322987188, Training Time: 11.50 seconds\n",
      "Epoch 261/300, Loss: 0.6948571806902166, Training Time: 11.56 seconds\n",
      "Epoch 271/300, Loss: 0.694212406378268, Training Time: 11.55 seconds\n",
      "Epoch 281/300, Loss: 0.6960014184874952, Training Time: 11.44 seconds\n",
      "Epoch 291/300, Loss: 0.6907241918013224, Training Time: 11.52 seconds\n",
      "Weight Decay Training Time: 3461.69 seconds\n",
      "Weight Decay Final Training Loss: 0.6964692298865989\n",
      "Weight Decay Final Validation Accuracy: 67.45%\n"
     ]
    }
   ],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, inputs, outputs, stride=1):\n",
    "        super(Block, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inputs, outputs, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(outputs)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(outputs, outputs, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(outputs)\n",
    "        self.downsample = nn.Sequential()\n",
    "        if stride != 1 or inputs != outputs:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(inputs, outputs, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(outputs),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += self.downsample(identity)\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet10(nn.Module):\n",
    "    def __init__(self, block, layers):\n",
    "        super(ResNet10, self).__init__()\n",
    "        self.inputs = 16\n",
    "        self.conv1 = nn.Conv2d(3, self.inputs, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(self.inputs)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.blocks = self._make_layers(block, layers)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(self.inputs, 10)\n",
    "\n",
    "    def _make_layers(self, block, layers):\n",
    "        layers_list = []\n",
    "        for layer in layers:\n",
    "            layers_list.append(block(self.inputs, self.inputs, stride=1))\n",
    "        return nn.Sequential(*layers_list)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.blocks(x)\n",
    "        x = self.avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "model = ResNet10(Block, [2, 2, 2]).to(device)  \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 300\n",
    "total_start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    losses = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "        \n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {losses/len(train_loader)}, Training Time: {training_time:.2f} seconds')\n",
    "\n",
    "total_end_time = time.time()\n",
    "total_training_time = total_end_time - total_start_time\n",
    "\n",
    "# Validation\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in valid_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "valid_accuracy = correct / total\n",
    "\n",
    "print(f'Weight Decay Training Time: {total_training_time:.2f} seconds')\n",
    "print(f'Weight Decay Final Training Loss: {losses / len(train_loader)}')\n",
    "print(f'Weight Decay Final Validation Accuracy: {valid_accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31a3b8f-a67a-4bb0-b1ae-b9618d67af6e",
   "metadata": {},
   "source": [
    "Dropout with p=0.3 ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e61785d-3e2b-47da-8d0a-0ea6fb5e953c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300, Loss: 2.599541852236404, Training Time: 11.56 seconds\n",
      "Epoch 11/300, Loss: 2.599963320795532, Training Time: 11.97 seconds\n",
      "Epoch 21/300, Loss: 2.599567667602578, Training Time: 14.24 seconds\n",
      "Epoch 31/300, Loss: 2.599937772506948, Training Time: 14.24 seconds\n",
      "Epoch 41/300, Loss: 2.5994041225184565, Training Time: 14.25 seconds\n",
      "Epoch 51/300, Loss: 2.599738048775422, Training Time: 14.24 seconds\n",
      "Epoch 61/300, Loss: 2.600074477207935, Training Time: 14.26 seconds\n",
      "Epoch 71/300, Loss: 2.5998129808079558, Training Time: 14.10 seconds\n",
      "Epoch 81/300, Loss: 2.5995759262758145, Training Time: 14.21 seconds\n",
      "Epoch 91/300, Loss: 2.600000722938791, Training Time: 14.23 seconds\n",
      "Epoch 101/300, Loss: 2.5996569949952537, Training Time: 14.24 seconds\n",
      "Epoch 111/300, Loss: 2.5995613775594766, Training Time: 14.25 seconds\n",
      "Epoch 121/300, Loss: 2.5996320329968583, Training Time: 14.26 seconds\n",
      "Epoch 131/300, Loss: 2.5998469557603605, Training Time: 14.25 seconds\n",
      "Epoch 141/300, Loss: 2.5998797130096905, Training Time: 14.25 seconds\n",
      "Epoch 151/300, Loss: 2.60006527248246, Training Time: 14.25 seconds\n",
      "Epoch 161/300, Loss: 2.5995265035068287, Training Time: 14.25 seconds\n",
      "Epoch 171/300, Loss: 2.5999736688326083, Training Time: 14.24 seconds\n",
      "Epoch 181/300, Loss: 2.599781936391845, Training Time: 14.22 seconds\n",
      "Epoch 191/300, Loss: 2.599928879371994, Training Time: 14.23 seconds\n",
      "Epoch 201/300, Loss: 2.5996248645855644, Training Time: 14.25 seconds\n",
      "Epoch 211/300, Loss: 2.5997090821375934, Training Time: 14.24 seconds\n",
      "Epoch 221/300, Loss: 2.5998041327956996, Training Time: 14.25 seconds\n",
      "Epoch 231/300, Loss: 2.5992736115175137, Training Time: 14.24 seconds\n",
      "Epoch 241/300, Loss: 2.6000404876211416, Training Time: 14.23 seconds\n",
      "Epoch 251/300, Loss: 2.599800767800997, Training Time: 14.24 seconds\n",
      "Epoch 261/300, Loss: 2.599096205838196, Training Time: 14.23 seconds\n",
      "Epoch 271/300, Loss: 2.5997848827820604, Training Time: 14.21 seconds\n",
      "Epoch 281/300, Loss: 2.5997701605872425, Training Time: 14.24 seconds\n",
      "Epoch 291/300, Loss: 2.599772833802206, Training Time: 14.26 seconds\n",
      "Dropout Training Time: 4240.97 seconds\n",
      "Dropout Final Training Loss: 2.5998836838071\n",
      "Dropout Final Validation Accuracy: 9.84%\n"
     ]
    }
   ],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, inputs, outputs, stride=1, dropout_prob=0.3):\n",
    "        super(Block, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inputs, outputs, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(outputs)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "        self.conv2 = nn.Conv2d(outputs, outputs, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(outputs)\n",
    "        self.downsample = nn.Sequential()\n",
    "        if stride != 1 or inputs != outputs:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(inputs, outputs, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(outputs),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += self.downsample(identity)\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "# Initialize the model with Dropout\n",
    "model = ResNet10(Block, [2, 2, 2]).to(device)  \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_dropout = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Training loop\n",
    "epochs = 300\n",
    "total_start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    losses = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "        \n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {losses/len(train_loader)}, Training Time: {training_time:.2f} seconds')\n",
    "\n",
    "total_end_time = time.time()\n",
    "total_training_time = total_end_time - total_start_time\n",
    "\n",
    "# Validation\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in valid_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "valid_accuracy = correct / total\n",
    "\n",
    "print(f'Dropout Training Time: {total_training_time:.2f} seconds')\n",
    "print(f'Dropout Final Training Loss: {losses / len(train_loader)}')\n",
    "print(f'Dropout Final Validation Accuracy: {valid_accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084e9e87-eb06-488a-aba0-a4f4cb8cd6f1",
   "metadata": {},
   "source": [
    "Batch Normalization ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76a9a1f9-82d8-4389-bfad-3b34d8db7623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300, Loss: 1.6468335275759782, Training Time: 19.18 seconds\n",
      "Epoch 11/300, Loss: 0.8638069114035658, Training Time: 19.01 seconds\n",
      "Epoch 21/300, Loss: 0.7471922308282779, Training Time: 19.11 seconds\n",
      "Epoch 31/300, Loss: 0.679054518146893, Training Time: 19.07 seconds\n",
      "Epoch 41/300, Loss: 0.6385786998683535, Training Time: 19.11 seconds\n",
      "Epoch 51/300, Loss: 0.6039083092413899, Training Time: 18.90 seconds\n",
      "Epoch 61/300, Loss: 0.5771825313187011, Training Time: 19.13 seconds\n",
      "Epoch 71/300, Loss: 0.5634580402423048, Training Time: 19.14 seconds\n",
      "Epoch 81/300, Loss: 0.5423234517083448, Training Time: 19.00 seconds\n",
      "Epoch 91/300, Loss: 0.527406616931986, Training Time: 19.21 seconds\n",
      "Epoch 101/300, Loss: 0.5046544743849494, Training Time: 19.14 seconds\n",
      "Epoch 111/300, Loss: 0.497510135211908, Training Time: 19.08 seconds\n",
      "Epoch 121/300, Loss: 0.48644770363636336, Training Time: 19.10 seconds\n",
      "Epoch 131/300, Loss: 0.4772624688227768, Training Time: 18.98 seconds\n",
      "Epoch 141/300, Loss: 0.4678489052311844, Training Time: 19.00 seconds\n",
      "Epoch 151/300, Loss: 0.4543661208789977, Training Time: 18.99 seconds\n",
      "Epoch 161/300, Loss: 0.45181810690085294, Training Time: 18.88 seconds\n",
      "Epoch 171/300, Loss: 0.4401057287288444, Training Time: 19.33 seconds\n",
      "Epoch 181/300, Loss: 0.430488843537505, Training Time: 18.95 seconds\n",
      "Epoch 191/300, Loss: 0.4234191092955487, Training Time: 19.05 seconds\n",
      "Epoch 201/300, Loss: 0.41773630695803393, Training Time: 18.97 seconds\n",
      "Epoch 211/300, Loss: 0.41509684344844133, Training Time: 19.05 seconds\n",
      "Epoch 221/300, Loss: 0.4071156056145268, Training Time: 18.99 seconds\n",
      "Epoch 231/300, Loss: 0.40544630179319846, Training Time: 19.02 seconds\n",
      "Epoch 241/300, Loss: 0.3978941470498929, Training Time: 18.95 seconds\n",
      "Epoch 251/300, Loss: 0.3909657419375751, Training Time: 18.94 seconds\n",
      "Epoch 261/300, Loss: 0.3898788930567177, Training Time: 18.91 seconds\n",
      "Epoch 271/300, Loss: 0.38919074527557246, Training Time: 19.28 seconds\n",
      "Epoch 281/300, Loss: 0.37760829853127376, Training Time: 19.07 seconds\n",
      "Epoch 291/300, Loss: 0.37806852901225807, Training Time: 19.01 seconds\n",
      "Batch Normalization Training Time: 18.96 seconds\n",
      "Batch Normalization Final Training Loss: 0.37296978168932676\n",
      "Batch Normalization Final Validation Accuracy: 71.54%\n"
     ]
    }
   ],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, inputs, outputs, stride=1):\n",
    "        super(Block, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inputs, outputs, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(outputs)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(outputs, outputs, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(outputs)\n",
    "        self.downsample = nn.Sequential()\n",
    "        if stride != 1 or inputs != outputs:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(inputs, outputs, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(outputs),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += self.downsample(identity)\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "# Initialize the model with Batch Normalization\n",
    "model = ResNet10(Block, [2, 2, 2]).to(device)  \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer= optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Training loop\n",
    "epochs = 300\n",
    "total_start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    losses = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "        \n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {losses/len(train_loader)}, Training Time: {training_time:.2f} seconds')\n",
    "\n",
    "total_end_time = time.time()\n",
    "total_training_time = total_end_time - total_start_time\n",
    "\n",
    "# Validation\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in valid_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "valid_accuracy = correct / total\n",
    "\n",
    "print(f'Batch Normalization Training Time: {total_training_time:.2f} seconds')\n",
    "print(f'Batch Normalization Final Training Loss: {losses / len(train_loader)}')\n",
    "print(f'Batch Normalization Final Validation Accuracy: {valid_accuracy * 100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
